import React, { useState, useRef, useCallback } from 'react';
import { useAuth } from '../../hooks/useAuth';
import { API_ENDPOINTS } from '../../config/api';

function AudioRecorder({ onTranscriptUpdate }) {
  const { userId } = useAuth();
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [isProcessing, setIsProcessing] = useState(false);
  const [language, setLanguage] = useState('ko');
  
  const mediaRecorderRef = useRef(null);
  const chunksRef = useRef([]);
  const intervalRef = useRef(null);
  const streamRef = useRef(null);
  const isProcessingRef = useRef(false);
  const lastTranscriptRef = useRef('');  // ì´ì „ ì „ì‚¬ ê²°ê³¼ ì¶”ì 

  const sendChunkForTranscription = useCallback(async (audioBlob) => {
    if (audioBlob.size < 1000) return;
    
    isProcessingRef.current = true;
    setIsProcessing(true);
    
    try {
      const formData = new FormData();
      formData.append('audio', audioBlob, 'chunk.webm');
      formData.append('user_id', userId);
      formData.append('language', language);

      const response = await fetch(API_ENDPOINTS.WHISPER_TRANSCRIBE, {
        method: 'POST',
        body: formData
      });

      const data = await response.json();
      
      if (data.success && data.transcript && data.transcript.trim()) {
        const fullText = data.transcript.trim();
        
        // ìƒˆë¡œìš´ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì´ì „ ê²°ê³¼ë³´ë‹¤ ê¸¸ë©´)
        if (fullText.length > lastTranscriptRef.current.length) {
          const newText = fullText.substring(lastTranscriptRef.current.length).trim();
          if (newText) {
            setTranscript(fullText);
            if (onTranscriptUpdate) {
              onTranscriptUpdate(fullText);
            }
          }
        }
        lastTranscriptRef.current = fullText;
      }
    } catch (error) {
      console.error('ì „ì‚¬ ì‹¤íŒ¨:', error);
    } finally {
      isProcessingRef.current = false;
      setIsProcessing(false);
    }
  }, [userId, language, onTranscriptUpdate]);

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true
        } 
      });
      streamRef.current = stream;
      
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });
      mediaRecorderRef.current = mediaRecorder;
      chunksRef.current = [];
      lastTranscriptRef.current = '';

      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };

      // 5ì´ˆë§ˆë‹¤ ì „ì²´ ë…¹ìŒì„ Whisperë¡œ ì „ì†¡
      intervalRef.current = setInterval(() => {
        if (chunksRef.current.length > 0 && !isProcessingRef.current) {
          // ì²­í¬ ì´ˆê¸°í™” ì•ˆ í•¨! ì „ì²´ë¥¼ ë³´ëƒ„
          const blob = new Blob(chunksRef.current, { type: 'audio/webm' });
          sendChunkForTranscription(blob);
        }
      }, 5000);

      mediaRecorder.start(1000);
      setIsRecording(true);
      setTranscript('');
      console.log('ğŸ¤ ë…¹ìŒ ì‹œì‘');

    } catch (error) {
      console.error('ë§ˆì´í¬ ì ‘ê·¼ ì‹¤íŒ¨:', error);
      alert('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.');
    }
  };

  const stopRecording = async () => {
    if (intervalRef.current) {
      clearInterval(intervalRef.current);
      intervalRef.current = null;
    }

    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }

    setIsRecording(false);

    // ìµœì¢… ì „ì‚¬
    if (chunksRef.current.length > 0) {
      const blob = new Blob(chunksRef.current, { type: 'audio/webm' });
      await sendChunkForTranscription(blob);
    }

    console.log('â¹ ë…¹ìŒ ì¢…ë£Œ');
  };

  const clearTranscript = () => {
    setTranscript('');
    lastTranscriptRef.current = '';
  };

  return (
    <div className="audio-recorder">
      <div className="recorder-header">
        <h3>ğŸ™ï¸ ì‹¤ì‹œê°„ ê°•ì˜ ë…¹ìŒ</h3>
        <p>5ì´ˆ ê°„ê²©ìœ¼ë¡œ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.</p>
      </div>

      <div className="language-selector">
        <label>ì–¸ì–´: </label>
        <select 
          value={language} 
          onChange={(e) => setLanguage(e.target.value)}
          disabled={isRecording}
        >
          <option value="ko">í•œêµ­ì–´</option>
          <option value="en">English</option>
        </select>
      </div>

      <div className="recorder-controls">
        {!isRecording ? (
          <button className="record-btn start" onClick={startRecording}>
            ğŸ¤ ë…¹ìŒ ì‹œì‘
          </button>
        ) : (
          <button className="record-btn stop" onClick={stopRecording}>
            â¹ ë…¹ìŒ ì¤‘ì§€
          </button>
        )}
        {transcript && (
          <button className="clear-btn" onClick={clearTranscript}>
            ğŸ—‘ï¸ ì´ˆê¸°í™”
          </button>
        )}
      </div>

      {isRecording && (
        <div className="recording-indicator">
          <span className="pulse-dot"></span>
          <span>ë…¹ìŒ ì¤‘... {isProcessing && '(ë³€í™˜ ì¤‘)'}</span>
        </div>
      )}

      <div className="transcript-box">
        <h4>ğŸ“ ë³€í™˜ëœ í…ìŠ¤íŠ¸</h4>
        <div className="transcript-content">
          {transcript || 'ë…¹ìŒì„ ì‹œì‘í•˜ë©´ ì—¬ê¸°ì— í…ìŠ¤íŠ¸ê°€ í‘œì‹œë©ë‹ˆë‹¤.'}
        </div>
      </div>

      <div className="transcript-stats">
        <span>ê¸€ì ìˆ˜: {transcript.length}ì</span>
      </div>
    </div>
  );
}

export default AudioRecorder;